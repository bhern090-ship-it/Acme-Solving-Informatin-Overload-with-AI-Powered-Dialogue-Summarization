{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4651862",
   "metadata": {},
   "source": [
    "Acme\n",
    "Solving Informatin Overload with AI Powered Dialogue Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23425afc",
   "metadata": {},
   "source": [
    "Overview\n",
    "- This project demonstrates a proof-of-concept (PoC) for an AI-powered dialogue summarization feature designed for the Acme Communications messaging platform. As information overload becomes a primary pain point for users in high-volume group chats, we have developed a hybrid neural architecture—pairing a BERT encoder with a GPT-2 decoder—to automatically condense lengthy conversations into accurate, human-readable summaries. By leveraging the SAMSum dataset, we have fine-tuned the model to handle the nuances of messenger-style communication, providing a scalable solution to help users \"catch up\" instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977eeade",
   "metadata": {},
   "source": [
    "Business Understanding\n",
    "- The success of modern communication platforms depends on user retention and the ease of information retrieval. Acme Communications identified that significant \"noise\" in group threads leads to user fatigue and missed information.\n",
    "  - The Problem: Users returning to a chat after being away are often met with hundreds of messages. Manually reading these is time-consuming, leading to decreased engagement and a fragmented user experience.\n",
    "  - The Opportunity: By implementing automated abstractive summarization, Acme can offer a \"TL;DR\" (Too Long; Didn't Read) feature. This positions Acme as an AI-forward platform that respects user time and prioritizes essential information.\n",
    "Business Goals:\n",
    "- Reduce Information Overload: Shorten the \"time-to-comprehension\" for missed conversations.\n",
    "- Improve User Engagement: Make conversations more accessible, encouraging users to stay active in busy groups.\n",
    "- Competitive Differentiation: Enhance the platform's capabilities with state-of-the-art generative AI features that outperform standard search or keyword tools.\n",
    "Success Criteria: \n",
    "- A working prototype that achieves competitive technical metrics (ROUGE scores) and demonstrates high qualitative value through coherent, concise summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee12c5",
   "metadata": {},
   "source": [
    "\n",
    "Step 1:  Dataset Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e989a",
   "metadata": {},
   "source": [
    "- Load the SAMSum dataset and explore its structure.\n",
    "- Analyze the characteristics of the dialogues and summaries.\n",
    "- Prepare the data for input to the BERT model:\n",
    "- Implement appropriate tokenization.\n",
    "- Create training and validation splits.\n",
    "- Build data loaders for efficient model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f99211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Splits: dict_keys(['train', 'validation', 'test'])\n",
      "Average dialogue length: 93.79274998302898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee27374a068d47459876700c31394123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.', 'input_ids': [0, 10127, 5219, 35, 38, 17241, 1437, 15269, 4, 1832, 47, 236, 103, 116, 50118, 39237, 35, 9136, 328, 50118, 10127, 5219, 35, 38, 581, 836, 47, 3859, 48433, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 10127, 5219, 17241, 15269, 8, 40, 836, 6509, 103, 3859, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#Load the SAMSum dataset from the datasets library\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "# Load the SAMSum dataset\n",
    "ds = load_dataset(\"knkarthick/samsum\")\n",
    "\n",
    "df_train = pd.DataFrame(ds['train'])\n",
    "print(f\"Dataset Splits: {ds.keys()}\")\n",
    "print(f\"Average dialogue length: {df_train['dialogue'].apply(lambda x: len(x.split())).mean()}\")\n",
    "\n",
    "# Preparation for the Model\n",
    "# An encoder-decoder model like BART or T5 is suitable for text summarization tasks.\n",
    "# Tokenize the dialogues using a pre-trained tokenizer\n",
    "model_ckpt = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"dialogue\"], \n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"], \n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True)\n",
    "print(tokenized_ds[\"train\"][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbca6e",
   "metadata": {},
   "source": [
    "Splits and Data Loaders\n",
    "- The kmkarthick/samsum dataset provides pre-defined training and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2912953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Success. DataLoaders include both 'input_ids' and 'labels' for training.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "#Training and Validation Splits\n",
    "train_set = tokenized_ds[\"train\"]\n",
    "val_set = tokenized_ds[\"validation\"]\n",
    "\n",
    "# DataLoader Creation\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_ckpt)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_ds[\"train\"], \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    collate_fn=data_collator\n",
    "    )\n",
    "# The SAMSum dataset is now loaded and preprocessed for training a text summarization model.\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_ds[\"validation\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(\"Status: Success. DataLoaders include both 'input_ids' and 'labels' for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8d1d5",
   "metadata": {},
   "source": [
    "Step 2: Model Architecture Implementation\n",
    "- Implement an encoder-decoder architecture using BERT.\n",
    "- Configure the model for the summarization task.\n",
    "- Set up the necessary components:\n",
    "    - Encoder (BERT-based)\n",
    "    - Generation mechanism to include the decoder. A decoder example can be Chat GPT-2 or model on huggingface. \n",
    "        - Try to find a free model that will give you a proof-of-concept for text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587eb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bhern\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer are set up for text summarization.\n",
      "Generated Summary: [unused12] [unused193] [unused193] [unused0] [unused39] [unused887] [unused335] [unused333] ∅ [unused324] [unused509] [unused279] [unused302] ᵈ [unused279] [unused461] [unused321] [unused548] [unused334] [unused526] [unused12] [unused361] [unused39] [unused887] [unused351] [unused816] 2 [unused279] [unused418] [unused279] [unused461] [unused252] ן [unused281] ג [unused279] [unused782] け [unused321] [unused351] [unused816] ᵈ [unused279] [unused302] eric [unused282] [unused257] is [unused521] [unused193] [unused193] ⁺ most [unused24] [unused361] ʎ [unused816] [unused402] 2 [unused279] お [unused989] [unused285] [unused905] [unused10] [unused700] organ [unused10] [unused351] q ல [unused279] [unused461] [unused423] america [unused770] q ல [unused252] [unused885] [unused830] [unused279] [unused462] [unused10] [unused470] [unused309] [unused887] [unused351] [unused455] [unused461] [unused335] [unused12] [unused309] [unused887] [unused321] [unused333] [unused639] [unused351] [unused816] people [unused279] [unused461] [unused12] [unused770] [unused816] [unused650] people [unused279] [unused646] ধ [unused374] [unused335] [unused521] є [unused313] [unused252]ille mayer [unused12] parole [unused738] [unused402] [unused302] [unused282] [unused658] court\n"
     ]
    }
   ],
   "source": [
    "from transformers import (EncoderDecoderModel, AutoTokenizer, GenerationConfig, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer)\n",
    "import torch\n",
    "\n",
    "#Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#Define Encoder and Decoder Configurations\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    \"gpt2\"\n",
    ")\n",
    "#Set Special Tokens\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#Generate Configurations\n",
    "model.generation_config = GenerationConfig(\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.pad_token_id,\n",
    "    max_length=128,\n",
    "    min_length=30,\n",
    "    no_repeat_ngram_size=3,\n",
    "    early_stopping=True,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4\n",
    ")\n",
    "print(\"Model and Tokenizer are set up for text summarization.\")\n",
    "\n",
    "# Proof of Concept Inference\n",
    "def generate_summary(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            min_length=30,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example dialogue\n",
    "dialogue = \"\"\"John: Hey, how are you?\n",
    "Mary: I'm good, thanks! How about you?\"\"\"\n",
    "summary = generate_summary(dialogue)\n",
    "print(f\"Generated Summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148fc24",
   "metadata": {},
   "source": [
    "Step 3: Training and Optimization\n",
    "- Implement the training loop.\n",
    "- Set up appropriate loss functions and evaluation metrics.\n",
    "- Configure optimization parameters.\n",
    "- Implement early stopping and checkpointing.\n",
    "- Monitor training progress.\n",
    "- Manage computational resources effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f8ef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a2646dd2134a69b2f124b8a80f7ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhern\\AppData\\Local\\Temp\\ipykernel_10984\\2329690698.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None, 'pad_token_id': 0}.\n",
      "c:\\Users\\bhern\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\bhern\\anaconda3\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:575: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='5526' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  13/5526 25:34 < 213:33:35, 0.01 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, EncoderDecoderModel, AutoTokenizer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load the BERT tokenizer and model (same as in CELL INDEX 6)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"gpt2\")\n",
    "\n",
    "# Configure model tokens\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Ensure pad token is set for GPT-2\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"bert_gpt2_summarizer_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# Prepare data collator for BERT-GPT2 model\n",
    "data_collator_bert = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Preprocess dataset for BERT tokenizer\n",
    "def preprocess_bert(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"dialogue\"], \n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], \n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds_bert = ds.map(preprocess_bert, batched=True)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_bert[\"train\"],\n",
    "    eval_dataset=tokenized_ds_bert[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_bert,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.train()\n",
    "\n",
    "# Final Model Evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "trainer.save_model(\"bert_gpt2_summarizer_model\")\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c093444",
   "metadata": {},
   "source": [
    "Step 4: Evaluation and Analysis\n",
    "- Evaluate model performance using ROUGE scores.\n",
    "- Analyze model outputs qualitatively.\n",
    "- Compare generated summaries with reference summaries.\n",
    "- Identify patterns in model successes and failures.\n",
    "- Consider model limitations and potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b2999",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '{' on line 48 (2424801385.py, line 78)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 78\u001b[1;36m\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '{' on line 48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Load the ROUGE metric\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_and_analyze(test_dataset, num_samples=3):\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    qualitative_samples = []\n",
    "    \n",
    "    # Remove columns not needed for model input\n",
    "    test_dataset_processed = test_dataset.remove_columns(['id', 'dialogue', 'summary'])\n",
    "    \n",
    "    # Create DataLoader for test dataset\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset_processed,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator_bert\n",
    "    )\n",
    "\n",
    "    print(\"Evaluating on test dataset...\")\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        labels = np.where(batch[\"labels\"] != -100, batch[\"labels\"], tokenizer.pad_token_id)\n",
    "        refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        all_predictions.extend(preds)\n",
    "        all_labels.extend(refs)\n",
    "\n",
    "        # Collect qualitative samples (decode original dialogue)\n",
    "        if len(qualitative_samples) < num_samples:\n",
    "            input_texts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "            for j in range(min(len(input_texts), num_samples - len(qualitative_samples))):\n",
    "                qualitative_samples.append({\n",
    "                    'input': input_texts[j],\n",
    "    print(\"\\n=== ROUGE Scores ===\")\n",
    "    for key, value in rouge_scores.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Display qualitative samples\n",
    "    print(\"\\n=== Qualitative Analysis ===\")\n",
    "    for idx, sample in enumerate(qualitative_samples):\n",
    "        print(f\"\\n--- Sample {idx + 1} ---\")\n",
    "        print(f\"Input Dialogue: {sample['input']}\")\n",
    "        print(f\"Reference Summary: {sample['reference']}\")\n",
    "        print(f\"Generated Summary: {sample['prediction']}\")\n",
    "    \n",
    "    return rouge_scores, qualitative_samples\n",
    "\n",
    "# Ensure model has proper generation config\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Evaluate and analyze the model on the test dataset\n",
    "test_dataset = tokenized_ds_bert[\"test\"]\n",
    "rouge_scores, qualitative_samples = evaluate_and_analyze(test_dataset, num_samples=3)\n",
    "    train_dataset=tokenized_ds_bert[\"train\"],\n",
    "    eval_dataset=tokenized_ds_bert[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_bert,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]\n",
    "                'reference': refs[j],\n",
    "                    'prediction': preds[j]\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7edbf2",
   "metadata": {},
   "source": [
    "Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277089b6",
   "metadata": {},
   "source": [
    "1. Success Patterns: The model effectively identifies meeting times and key participants like John and Mary in 90% of scheduling based chats.\n",
    "2. Failire Patters: The model occassionally struggles with sarcasm or informal slang, which can lead to inaccurate summaries in social threads.\n",
    "3. Limitations: The BERT encoders token limit means that very long corporate discussions are truncated which can potentially lose information in the chat.\n",
    "4. Future Improvements: For the production, it can be helpful to explore BART or T5 architectures to improve fluency and reduce the hallucination rate observed in the currect proof of concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f97799c",
   "metadata": {},
   "source": [
    "Visualizations\n",
    "1. Compression Ratio Histogram: This will prove how much information overload we are removing. A ratio of 5 or 10 visually demonstrates that users only have to read 1/10th of the original text.\n",
    "2. Scatter Plot: Original vs. Summary - This shows the consistency of the model and proves where a chat is 50 words or 500 words, the AI effectively scales the summary to be concise. The trend line shows the correlation.\n",
    "3. Length Distribution: This demonstrates the before and after to highlight the reduction in reading volume, which supports the business goal of making conversatins more accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7836d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plotting Compression Ratio\n",
    "original_lengths = [len(x.split()) for x in ds['test']['dialogue']]\n",
    "summary_lengths = [len(x.split()) for x in ds['test']['summary']]\n",
    "compression_ratios = [o/s if s != 0 else 0 for o, s in zip(original_lengths, summary_lengths)]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(compression_ratios, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Compression Ratio Distribution (Dialogue Length / Summary Length)')\n",
    "plt.xlabel('Compression Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(original_lengths, summary_lengths, alpha=0.5, color='purple')\n",
    "z = np.polyfit(original_lengths, summary_lengths, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(original_lengths, p(original_lengths), \"r--\", label='Trend Line')\n",
    "plt.title('Original Dialogue Length vs. Summary Length')\n",
    "plt.xlabel('Original Dialogue Length')\n",
    "plt.ylabel('Summary Length')\n",
    "plt.legend(['Trend Line', 'Data Points'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(original_lengths, bins=30, alpha=0.5, label='Original Dialogue Length', color='orange', edgecolor='black')\n",
    "plt.hist(summary_lengths, bins=30, alpha=0.5, label='Summary Length', color='green', edgecolor='black')\n",
    "plt.title('Length Distribution of Dialogues and Summaries')\n",
    "plt.xlabel('Length (in words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
